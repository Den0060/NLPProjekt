{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938d8e46-59c6-439f-9ea8-87541c13596d",
   "metadata": {},
   "source": [
    "# Summarizer\n",
    "\n",
    "Erstellt von: Enes Yilmaz, Dennis Waltemathe, Demir Dolovac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8526008-683b-4d67-afd6-acdc6ec950f1",
   "metadata": {},
   "source": [
    "## Einleitung\n",
    "\n",
    "Das Zusammenfassen von Texten ist eine spannende Herausforderung. Besonders bei politischen Reden, wie bei den Bundestagsdebatten gibt es viele Details, was die Generierung von Zusammenfassungen schwierig macht.\n",
    "\n",
    "Um dieses Problem zu l√∂sen, haben wir uns einen passenden Datensatz gesucht, welcher die Bundestagsdebatten von 1949 bis 2021 enth√§lt. Allerdings gab es dabei ein Problem und zwar existierten keine Referenzzusammenfassungen, die als Vergleich oder Traingsdaten genutzt werden konnten.\n",
    "Leider konnten wir auch keinen anderen Datensatz finden, der solche Zusammenfassungen enth√§lt. Deshalb haben wir f√ºr einen kleinen zuf√§lligen Teil des Datensatzes, Zusammenfassungen generieren lassen. Diese haben wir in einer neuen CSV-Datei gespeichert, um unser Modell darauf zu trainieren.\n",
    "\n",
    "Im folgenden Bericht werden wir erkl√§ren, welche Arten von Zusammenfassungen es gibt, wie wir die Zusammenfassungen erstellt haben, wie das Modell trainiert wurde und welche Ergebnisse wir erzielt haben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a5a4b1-149b-426e-b4c1-ab8d8acb5bf8",
   "metadata": {},
   "source": [
    "## Zusammenfassungstypen\n",
    "\n",
    "Es gibt zwei Arten von Textzusammenfassungen, die **abstraktive** und **extraktive** Zusammenfassung.\n",
    "Bei der abstraktiven Zusammenfassung wird die Kernaussage eines Textes mit eigenen Worten zusammengefasst. \n",
    "Das Modell erstellt also eine Zusammenfassung, die nicht 1:1 aus dem urspr√ºnglichen Text entnommen ist. Diese Methode √§hnelt der menschlichen Zusammenfassung.\n",
    "Im Gegensatz dazu werden beim extraktiven Vorgehen die wichtigsten S√§tze aus dem Originaltext extrahiert und in eine logische Reihenfolge gebracht. Es werden also keine neuen S√§tze generiert, sondern nur relevante S√§tze des Originaltextes √ºbernommen.\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "Originaltext:<br>\n",
    "*‚ÄûDie Bundestagsdebatten sind ein zentraler Bestandteil der politischen Entscheidungsfindung in Deutschland. Sie erm√∂glichen es den Abgeordneten, ihre Positionen und √ºber wichtige Gesetzesvorhaben zu diskutieren.‚Äú*\n",
    "\n",
    "Extraktive Zusammenfassung:<br>\n",
    "*‚ÄûBundestagsdebatten sind ein zentraler Bestandteil der politischen Entscheidungsfindung in Deutschland, ihre Positionen und √ºber wichtige Gesetzesvorhaben zu diskutieren.‚Äú*\n",
    "\n",
    "Abstraktive Zusammenfassungen:<br>\n",
    "*‚ÄûBundestagsdebatten dienen der politischen Meinungsbildung und Gesetzesberatung.‚Äú*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09588e-d142-4185-ac9c-855a03fde7e6",
   "metadata": {},
   "source": [
    "## Datensatz\n",
    "\n",
    "F√ºr die Entwicklung und Evaluation unserer Datenmodelle nutzen wir den speeches.csv-Datensatz von Open Discourse [Open Discourse Dokumentation](https://open-discourse.github.io/open-discourse-documentation/1.1.0/index.html)\n",
    ". Dieser Datensatz enth√§lt s√§mtliche Redebeitr√§ge, die von 1949 bis 2021 im Deutschen Bundestag gehalten wurden, und bietet eine umfangreiche sowie gut strukturierte Grundlage f√ºr die Analyse politischer Sprache.\n",
    "\n",
    "Der Datensatz umfasst √ºber 900.000 Reden, erg√§nzt durch Metadaten wie Wahlperiode, Fraktion, Sprecher sowie das genaue Datum der Rede. <br>\n",
    "Die wichtigsten Merkmale des Datensatzes sind:<br>\n",
    "Klar definierte Strukturen: Die enthaltenen Metadaten erm√∂glichen eine gezielte Analyse basierend auf politischen Parteien, Legislaturperioden oder individuellen Abgeordneten.<br>\n",
    "√ñffentlich zug√§nglich und gut dokumentiert: Open Discourse stellt eine detaillierte Dokumentation bereit, die eine einfache Nutzung und Reproduzierbarkeit wissenschaftlicher Arbeiten sicherstellt.\n",
    "<br><br>\n",
    "Wir haben jedoch keine Referenzzusammenfassungen in diesem Datensatz, welche f√ºr das Trainieren und Testen des Summarizers ben√∂tigt werden. Deshalb zeigen wir im Folgenden, wie wir dieses Problem f√ºr einen kleinen Teil der speeches.csv gel√∂st haben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153de54-a85a-4948-8c89-4d4070a454ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "Zu erst laden wir die speeches.csv:"
   ]
  },
  {
   "cell_type": "code",
   "id": "529327be-47f0-4cb4-9547-a0798e3f7e4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:12:32.000874Z",
     "start_time": "2025-02-13T17:12:15.609669Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "csv_datei = \"data_summarizer/speeches.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(csv_datei)\n",
    "\n",
    "speech_contents = df[\"speechContent\"]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "cd76e6a8-07a7-4458-a195-575200bdcf26",
   "metadata": {},
   "source": [
    "Nun bereiten wir die Originaltexte etwas auf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3ebe52-f86d-419d-97e2-2add9ab032e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \").strip()  # Zeilenumbr√ºche entfernen\n",
    "    text = re.sub(r\"\\{\\d+\\}\", \"\", text)  # Entfernt die {0}, {1}, {2}, etc. aus den Texten\n",
    "    text = text.lower()  # Alles in Kleinbuchstaben umwandeln\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d6bf5-3d54-4750-ab34-16bbc05d661b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Als n√§chstes filtern wir alle Texte mit mind. 20 W√∂rtern und w√§hlen zuf√§llig 4000 Texte, welche wir dann in die neue CSV speichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c497b59-a95a-4a39-b6d4-bd0afb7938b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtert Texte mit mindestens 20 W√∂rtern\n",
    "filtered_texts = speech_contents[speech_contents.str.split().str.len() >= 20].apply(clean_text)\n",
    "\n",
    "# Zuf√§llig 4000 Texte ausw√§hlen\n",
    "random_texts = filtered_texts.sample(n=4000, random_state=42)\n",
    "\n",
    "# Speichert die gefilterten Texte in eine neue CSV-Datei\n",
    "random_texts.to_csv(\"data_summarizer/filtered_speeches.csv\", index=False)   #ACHTUNG: Hier wird dann √ºberschrieben, am besten ein neuen Dateinamen eingeben!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e2da10-694f-45bc-a3cb-ee47290c017e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speechContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>herr pr√§sident! meine sehr verehrten damen und...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>das ist eine subjektive bewertung, frau kolleg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>herr kollege czaja, ist ihnen entgangen, da√ü i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>herr staatsminister, war es nicht, da die verh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aber, herr kollege schiller, wollen sie nicht ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>das wort hat nun der innenminister von badenw√º...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>herr staatssekret√§r, ist es eigentlich zul√§ssi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>frau abgeordnete, in dem bericht finden sie di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sie m√ºssen sehen, dass der bundesverkehrswegep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>herr staatssekret√§r, im hinblick darauf, da√ü g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       speechContent\n",
       "0  herr pr√§sident! meine sehr verehrten damen und...\n",
       "1  das ist eine subjektive bewertung, frau kolleg...\n",
       "2  herr kollege czaja, ist ihnen entgangen, da√ü i...\n",
       "3  herr staatsminister, war es nicht, da die verh...\n",
       "4  aber, herr kollege schiller, wollen sie nicht ...\n",
       "5  das wort hat nun der innenminister von badenw√º...\n",
       "6  herr staatssekret√§r, ist es eigentlich zul√§ssi...\n",
       "7  frau abgeordnete, in dem bericht finden sie di...\n",
       "8  sie m√ºssen sehen, dass der bundesverkehrswegep...\n",
       "9  herr staatssekret√§r, im hinblick darauf, da√ü g..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.read_csv(\"data_summarizer/filtered_speeches.csv\")\n",
    "\n",
    "df_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d551fd-e611-498f-9edf-b55f7c1fa12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Zeilen in alter CSV: 907644\n",
      "Anzahl der Zeilen in neuer CSV: 4000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Anzahl der Zeilen in alter CSV: {len(df)}\")\n",
    "print(f\"Anzahl der Zeilen in neuer CSV: {len(df_new)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02aea5c-3477-4108-a728-3bc76f7bb80a",
   "metadata": {},
   "source": [
    "Anschlie√üend generieren wir f√ºr die 4000 Texte jeweils eine Referenzzusammenfassung, dazu verwenden wir **gpt-4o-mini**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988f13e-f1d7-489e-859b-fc6d6b67b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "openai.api_key = \"KEY\"\n",
    "\n",
    "file_path = \"data_summarizer/filtered_speeches.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def generate_summary_gpt(text):\n",
    "    \"\"\"\n",
    "    Erstellt f√ºr jeden Originaltext aus der filtered_speeches.csv eine Referenzzusammenfassung.\n",
    "    :param text: Der Originaltext\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Erstelle eine pr√§gnante, abstraktive Zusammenfassung der folgenden Bundestagsrede, die Zusammenfassung soll k√ºrzer sein als der Originaltext:\n",
    "    \n",
    "    Originaltext: \"{text}\"\n",
    "    \n",
    "    Zusammenfassung: (Bitte beende die Zusammenfassung in vollst√§ndigen S√§tzen.)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Du bist ein Experte f√ºr Textzusammenfassungen.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=250,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der OpenAI-Anfrage: {e}\")\n",
    "        return \"Fehler bei der Generierung\"\n",
    "\n",
    "summaries = []\n",
    "for text in tqdm(df[\"speechContent\"], desc=\"Generiere abstraktive Zusammenfassungen mit GPT-4\", unit=\"Text\"):\n",
    "    summaries.append(generate_summary_gpt(str(text)))\n",
    "\n",
    "# Neue summaries Spalte f√ºr die Referenzzusammenfassungen\n",
    "df[\"summaries\"] = summaries\n",
    "\n",
    "# Datei speichern\n",
    "output_file = \"data_summarizer/filtered_speeches_with_summaries.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Zusammenfassungen gespeichert in '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "735e48ea-780d-4730-8b03-6adaf5c621dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['speechContent', 'summaries'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speechContent</th>\n",
       "      <th>summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>herr pr√§sident! meine sehr verehrten damen und...</td>\n",
       "      <td>In seiner Rede kritisiert der Abgeordnete die ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>das ist eine subjektive bewertung, frau kolleg...</td>\n",
       "      <td>Die Rede betont, dass die Erstellung des Bewer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>herr kollege czaja, ist ihnen entgangen, da√ü i...</td>\n",
       "      <td>Der Redner weist Herrn Czaja darauf hin, dass ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>herr staatsminister, war es nicht, da die verh...</td>\n",
       "      <td>Der Redner fordert den Staatsminister auf, die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aber, herr kollege schiller, wollen sie nicht ...</td>\n",
       "      <td>Der Redner weist darauf hin, dass die Prognose...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       speechContent   \n",
       "0  herr pr√§sident! meine sehr verehrten damen und...  \\\n",
       "1  das ist eine subjektive bewertung, frau kolleg...   \n",
       "2  herr kollege czaja, ist ihnen entgangen, da√ü i...   \n",
       "3  herr staatsminister, war es nicht, da die verh...   \n",
       "4  aber, herr kollege schiller, wollen sie nicht ...   \n",
       "\n",
       "                                           summaries  \n",
       "0  In seiner Rede kritisiert der Abgeordnete die ...  \n",
       "1  Die Rede betont, dass die Erstellung des Bewer...  \n",
       "2  Der Redner weist Herrn Czaja darauf hin, dass ...  \n",
       "3  Der Redner fordert den Staatsminister auf, die...  \n",
       "4  Der Redner weist darauf hin, dass die Prognose...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_speeches_with_summaries =  pd.read_csv(\"data_summarizer/filtered_speeches_with_summaries.csv\")\n",
    "print(df_speeches_with_summaries.columns)\n",
    "df_speeches_with_summaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd6be5f-b248-41cf-b8d3-42a8d1b4b66f",
   "metadata": {},
   "source": [
    "## Finetuning eines Modells\n",
    "\n",
    "Im Folgenden erkl√§ren wir, wie wir Modelle gefinetuned haben. Dazu haben wir vorher die **filtered_speeches** in eine **train.csv** und **test.csv** aufgeteilt (75% train, 25% test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28c4ad-4a61-48e2-9dbd-c8efe4e53162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Lade deine CSV-Datei\n",
    "df = pd.read_csv(\"data_summarizer/filtered_speeches_with_summaries.csv\") \n",
    "\n",
    "# Pr√ºfe, ob die relevanten Spalten vorhanden sind\n",
    "if \"speechContent\" not in df.columns or \"summaries\" not in df.columns:\n",
    "    raise ValueError(\"Die CSV-Datei muss die Spalten 'speechContent' und 'summaries' enthalten.\")\n",
    "\n",
    "# Teile die Daten auf (75% Training, 25% Test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Speichere die neuen Dateien\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(f\"Trainingsdaten: {len(train_df)} Zeilen\")\n",
    "print(f\"Testdaten: {len(test_df)} Zeilen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d853de64-da38-4f87-b28c-af99eeafd2da",
   "metadata": {},
   "source": [
    "Zu erst laden wir die Datens√§tze und legen die Teilmengen f√ºr das Training und Testen fest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4611cacc-742a-4354-a22b-c2c12a5e0696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"data_summarizer/train.csv\", \"test\": \"data_summarizer/test.csv\"})\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b7d54-22f7-4701-aabe-ab80de8f9bd4",
   "metadata": {},
   "source": [
    "Als n√§chstes laden wir den AutoTokenizer f√ºr ein vortrainiertes Modell und definieren eine Funktion, die Redebeitr√§ge (`speechContent`) und Zusammenfassungen (`summaries`) tokenisiert. Dabei werden die Eingaben auf eine maximale L√§nge gebracht, mit Padding erg√§nzt und abgeschnitten. Anschlie√üend mappen wir die Funktion auf den gesamten Datensatz, um tokenisierte Trainingsdaten zu erstellen.\n",
    "\n",
    "Wir m√∂chten das Modell [t5-small](https://huggingface.co/google-t5/t5-small) finetunen, welches bereits f√ºr die deutsche Sprache bzw. das Zusammenfassen deutscher Texte optimiert wurde. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "344aac2a-0e01-482c-9dbb-a5db054d8c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf9129e02cb40f59fb2298f064e4660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"t5-small\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"speechContent\"], \n",
    "        max_length=512, \n",
    "        padding=\"max_length\",  \n",
    "        truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"summaries\"], \n",
    "        max_length=150, \n",
    "        padding=\"max_length\",  # Auch hier Padding auf die maximale L√§nge\n",
    "        truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41717c75-4e34-4574-a052-8a239262165e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221960f3-4e47-44fd-bbde-d900f026a1ce",
   "metadata": {},
   "source": [
    "Nun trainieren wir das Modell, dabei definieren wir die Trainingsparameter und erstellen einen Trainer, die das Modell fintuned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "123bc145-6d24-480b-a754-456373982392",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_104/3582634350.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='752' max='752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [752/752 07:49, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.100400</td>\n",
       "      <td>1.725781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.919500</td>\n",
       "      <td>1.549682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.741400</td>\n",
       "      <td>1.480842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.688300</td>\n",
       "      <td>1.445464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.576100</td>\n",
       "      <td>1.422272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.609400</td>\n",
       "      <td>1.410084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.542100</td>\n",
       "      <td>1.402987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.618900</td>\n",
       "      <td>1.400709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=752, training_loss=1.9296705386740096, metrics={'train_runtime': 473.1722, 'train_samples_per_second': 50.721, 'train_steps_per_second': 1.589, 'total_flos': 3248203235328000.0, 'train_loss': 1.9296705386740096, 'epoch': 8.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    do_train=True,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17818d40-aa1a-4f2d-81f6-25a72ff96bf2",
   "metadata": {},
   "source": [
    "Anschlie√üend speichern wir das Modell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "492fcf43-589e-46e6-a3df-a1db73185f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5_finetuned/summarizer_model/tokenizer_config.json',\n",
       " 't5_finetuned/summarizer_model/special_tokens_map.json',\n",
       " 't5_finetuned/summarizer_model/spiece.model',\n",
       " 't5_finetuned/summarizer_model/added_tokens.json',\n",
       " 't5_finetuned/summarizer_model/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"models_summarizer/t5_finetuned_abgabe_neu/summarizer_model\")  #Neu wurde hinzugef√ºgt, damit das bereits trainierte Modell nicht √ºberschrieben wird\n",
    "tokenizer.save_pretrained(\"models_summarizer/t5_finetuned_abgabe_neu/summarizer_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f882ac3c-0d87-485a-9ae6-95d3bc5a90bf",
   "metadata": {},
   "source": [
    "## ROUGE-Score\n",
    "Zur Bewertung der Modelle haben wir den ROUGE-Score (Recall-Oriented Unterstudy for Gisting Evaluation) verwendet, dies ist eine Metrik, welche zur Bewertung der Qualit√§t von autmatisch generierter Texte wie zum Beispiel Zusammenfassungen verwendet wird. \n",
    "Der ROUGE-Score vergleicht die vom Modell erzeugte Zusammenfassung mit einer Referenzzusammenfassung, indem er die √úbereinstimmungen von n-Grammen (Wortsequenzen) misst. \n",
    "So weist ein h√∂her ROUGE-Score auf eine h√∂here √Ñhnlichkeit zwischen der generierten Zusammenfassung und der Referenzzusammenfassung hin.\n",
    "\n",
    "Um die Qualit√§t der generierten Zusammenfassungen genauer bewerten zu k√∂nnen, gibt es verschiedene ROUGE-Varianten, welche verschiedene Teile der inhaltlichen √úbereinstimmung ber√ºcksichtigen:\n",
    "\n",
    "**<u>ROUGE-N</u>** <br>\n",
    "Diese Variante misst √úberlappung von n-Grammen (Sequenzen von n aufeinanderfolgenden W√∂rtern) zwischen der generierten und Referenzzusammenfassung.\n",
    "So w√ºrde beispielsweise der ROUGE-1 Score die √úbereinstimmung einzelner W√∂rter (Unigramme) messen und der ROUGE-2 Score die √úbereinstimmung von Wortpaaren (Bigramme).\n",
    "\n",
    "Beispiel:<br>\n",
    "Referenz: *‚ÄûDie Katze sitzt auf dem Stuhl‚Äú*<br>\n",
    "Generiert: *‚ÄûDer Hund schl√§ft auf dem Stuhl‚Äú*\n",
    "\n",
    "Die gemeinsamen Unigramme w√§ren hier: [‚Äûauf‚Äú, ‚Äûdem‚Äú, ‚ÄûStuhl‚Äú] und die gemeinsamen Bigramme w√§ren: [‚Äûauf dem‚Äú, ‚Äûdem Stuhl‚Äú]\n",
    "\n",
    "**<u>ROUGE-L</u>** <br>\n",
    "Eine weitere Variante basiert auf der l√§ngsten gemeinsamen Teilsequenz (LCS). Hier wird die l√§ngste gemeinsame Teilsequenze der beiden Zusammenfassungen gemessen, dabei m√ºssen die W√∂rter nicht unbedingt nebeneinander stehen.\n",
    "\n",
    "Beispiel:<br>\n",
    "Referenz: *‚ÄûDas Auto ist schnell und gro√ü.‚Äú*<br>\n",
    "Generiert: *‚ÄûDas rote Auto f√§hrt schnell.‚Äú*\n",
    "\n",
    "Die l√§ngste gemeinsame Teilsequenz w√§re hier ‚ÄûDas Auto schnell‚Äú.\n",
    "\n",
    "**<u>ROUGE-S</u>** <br>\n",
    "Die letzte ROUGE-Score Variante misst die √úbereinstimmung von Skip-Bigrammen, also Wortpaaren die in einer Reihenfolge vorkommen, aber nicht direkt nebeneinander stehen, dies erm√∂glicht es Wortpaare aus dem Referenztext zu identifizieren, die auch in der generierten Zusammenfassung vorkommen, dadurch wird der inhaltliche Kontext eines Textes erfasst, selbst wenn W√∂rter durch andere Begriffe getrennt ist.\n",
    "\n",
    "Beispiel: <br>\n",
    "Referenz: *‚ÄûDie Katze ist auf der Matte.‚Äú* <br>\n",
    "Generiert: *‚ÄûDie graue Katze springt herum.‚Äú*\n",
    "\n",
    "Im ROUGE-N w√ºrde es keine √úberlappungen geben, doch mittels Skip-Bigrammen w√ºrden hier ‚ÄûDie Katze‚Äú und ‚ÄûDie graue Katze‚Äú √ºbereinstimmen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f2187e-4813-491b-bdef-c1a707ee0791",
   "metadata": {},
   "source": [
    "## Berechnung der ROUGE-Scores\n",
    "\n",
    "Die Berechnung basiert auf den Konzepten der **Recall**, **Precsision** und **F1-Score**.\n",
    "So wird der Recall durch den Anteil der √ºberlappenden W√∂rtern aus der Referenz mit der generierten Zusammenfassung und der Gesamtanzahl der n-Gramme der Referenzzusammenfassung berechnet:\n",
    "\n",
    "$$recall = \\frac{gemeinsame\\ n-Gramme}{n-Gramme\\ in\\ der\\ Referenzzusammenfassung} $$\n",
    "\n",
    "Die Precision wird durch den Anteil der gemeinsamen n-Gramme und mit der Gesamtzahl der n-Gramme der generierten Zusammenfassung berechnet:\n",
    "\n",
    "$$precision = \\frac{gemeinsame\\ n-Gramme}{n-Gramme\\ in\\ der\\ generierten\\ Zusammenfassung} $$\n",
    "\n",
    "Um ein Gleichgewicht zwischen Recall und Precision zu finden wird der F1-Score folgenderma√üen berechnet:\n",
    "\n",
    "$$F1 = 2\\ x\\ \\frac{precision\\ x\\ recall}{precision\\ +\\ recall} $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b0ea5-e190-497d-82bb-ceb3cdff3a44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Finetuned vs. Original\n",
    "\n",
    "Nun vergleichen wir das originale Modell und das finedtuned Modell, dazu verwenden wir den soeben beschriebenen ROUGE-Score. Daf√ºr laden wir uns zuerst die Metrik runter:"
   ]
  },
  {
   "cell_type": "code",
   "id": "0a8c866d-48ea-48b1-af97-9916508cd49f",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-13T13:31:29.950013Z",
     "start_time": "2025-02-13T13:31:17.838050Z"
    }
   },
   "source": [
    "!pip install rouge-score\n",
    "#https://thepythoncode.com/article/calculate-rouge-score-in-python#setting-up-the-environment-for-python-implementation"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from nltk->rouge-score) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml): started\n",
      "  Building wheel for rouge-score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25025 sha256=140f2d7799bfd8e9f727c3be9a35ecf7053a3a037a378822f3d8af562a70d1fd\n",
      "  Stored in directory: c:\\users\\dolov\\appdata\\local\\pip\\cache\\wheels\\85\\9d\\af\\01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-2.1.0 rouge-score-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "864a6bdc-1513-46c5-ac13-b41587888b9c",
   "metadata": {},
   "source": [
    "Nun berechnen wir den ROUGE-Score, dazu wird ein Text aus der test.csv verwendet und von beiden Modellen zusammengefasst. Anschlie√üend werden beide Zusammenfassungen mit der Referenzzusammenfassung verglichen, was dann die ROUGE-Scores f√ºr Presicion, Recall und F1-Score liefert:"
   ]
  },
  {
   "cell_type": "code",
   "id": "825a00b8-f4ce-476b-9dd2-82b414070f0b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-13T13:33:04.528905Z",
     "start_time": "2025-02-13T13:32:59.355455Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "df_test_csv = pd.read_csv(\"data_summarizer/test.csv\")\n",
    "\n",
    "\n",
    "originaltext = df_test_csv[\"speechContent\"][0]\n",
    "referenztext = df_test_csv[\"summaries\"][0]\n",
    "\n",
    "# Finegetuntes Modell\n",
    "fn_summarizer = pipeline(\"summarization\", model=\"models_summarizer/t5_finetuned_abgabe/summarizer_model\", tokenizer=\"models_summarizer/t5_finetuned_abgabe/summarizer_model\")\n",
    "fn_zusammenfassung = fn_summarizer(originaltext, max_length=150, min_length=50, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "# Originales Modell\n",
    "orig_summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")\n",
    "orig_zusammenfassung = orig_summarizer(originaltext, max_length=150, min_length=50, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# ROUGE f√ºr finegetuntes Modell\n",
    "fn_rouge = scorer.score(referenztext, fn_zusammenfassung)\n",
    "\n",
    "# ROUGE f√ºr Basismodell\n",
    "orig_rouge = scorer.score(referenztext, orig_zusammenfassung)\n",
    "\n",
    "print(\"Fine-tuned Modell ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {fn_rouge['rouge1']}\")\n",
    "print(f\"ROUGE-2: {fn_rouge['rouge2']}\")\n",
    "print(f\"ROUGE-L: {fn_rouge['rougeL']}\")\n",
    "\n",
    "print(\"\\nOriginales Modell ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {orig_rouge['rouge1']}\")\n",
    "print(f\"ROUGE-2: {orig_rouge['rouge2']}\")\n",
    "print(f\"ROUGE-L: {orig_rouge['rougeL']}\")\n",
    "\n",
    "print()\n",
    "print(f\"Originaltext: {originaltext}\\n\")\n",
    "print(f\"Referenzzusammenfassung: {referenztext}\\n\")\n",
    "print(f\"Originales Modell: {orig_zusammenfassung}\\n\")\n",
    "print(f\"FN-Modell: {fn_zusammenfassung}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 94. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 94. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Modell ROUGE Scores:\n",
      "ROUGE-1: Score(precision=0.6774193548387096, recall=0.7777777777777778, fmeasure=0.7241379310344828)\n",
      "ROUGE-2: Score(precision=0.4666666666666667, recall=0.5384615384615384, fmeasure=0.5)\n",
      "ROUGE-L: Score(precision=0.5806451612903226, recall=0.6666666666666666, fmeasure=0.6206896551724138)\n",
      "\n",
      "Originales Modell ROUGE Scores:\n",
      "ROUGE-1: Score(precision=0.6176470588235294, recall=0.7777777777777778, fmeasure=0.6885245901639345)\n",
      "ROUGE-2: Score(precision=0.45454545454545453, recall=0.5769230769230769, fmeasure=0.5084745762711863)\n",
      "ROUGE-L: Score(precision=0.5294117647058824, recall=0.6666666666666666, fmeasure=0.5901639344262295)\n",
      "\n",
      "Originaltext: herr kollege berger, meine antwort ist kurz; sie lautet nein. die dislozierung der luftlandetruppen des warschauer paktes ist nach dem erkenntnisstand der bundesregierung nach wie vor unver√§ndert. es gibt auch keine anzeichen f√ºr m√∂gliche ver√§nderungen au√üerhalb des mbfr-reduzierungsraums.\n",
      "\n",
      "Referenzzusammenfassung: Die Bundesregierung sieht keine Ver√§nderungen bei der Dislozierung der Luftlandetruppen des Warschauer Paktes und erkennt auch keine Anzeichen f√ºr √Ñnderungen au√üerhalb des MBFR-Reduzierungsraums.\n",
      "\n",
      "Originales Modell: die dislozierung der luftlandetruppen des warschauer paktes ist nach dem erkenntnisstand der bundesregierung nach wie vor unver√§ndert. es gibt auch keine anzeichen f√ºr m√∂gliche ver√§nderungen au√üerhalb des mbfr-reduzierungsraums.\n",
      "\n",
      "FN-Modell: kollege berger hebt die Dislozierung der Luftlandetruppen des warschauer paktes nach dem Erkenntnisstand der Bundesregierung hervor und gibt keine Anzeichen f√ºr m√∂gliche Ver√§nderungen au√üerhalb des mbfr-reduzierungsraums.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "16a43ec9-0720-4003-931a-66225efe1ca3",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "**Vergleich mit anderen Modellen:**\n",
    "\n",
    "Dazu werden die finegetunten Modelle mit den originalen Modellen vergleichen. Die finegetunten Modelle sind unter (`mt5-small/summarizer_model`) (`t5_finetuned_abgabe/summarizer_model`) und (`bert_finetuned/summarizer_model`) zu finden.\n",
    "\n",
    "Originaltext: df_test_csv[\"speechContent\"][622]\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "\n",
    "<!-- Erste Tabelle -->\n",
    "<div>\n",
    "<p style=\"text-align: center;\"><strong>Finetuned Modelle</strong></p>\n",
    "<table>\n",
    "<tr>\n",
    "<th>ROUGE-SCORE</th><th>mt5-small</th><th>t5-small</th><th>mbart-large-50</th>\n",
    "</tr>\n",
    "<tr><td>R1-Precision</td><td>0.4848</td><td>0.6</td><td>0.7096</td></tr>\n",
    "<tr><td>R1-Recall</td><td>0.5925</td><td>0.5555</td><td>0.4313</td></tr>\n",
    "<tr><td>R1-F1</td><td>0.5333</td><td>0.5769</td><td>0.5365</td></tr>\n",
    "<tr><td>R2-Precision</td><td>0.3125</td><td>0.3333</td><td>0.3666</td></tr>\n",
    "<tr><td>R2-Recall</td><td>0.3846</td><td>0.3076</td><td>0.22</td></tr>\n",
    "<tr><td>R2-F1</td><td>0.3448</td><td>0.32</td><td>0.2749</td></tr>\n",
    "<tr><td>RL-Precision</td><td>0.3636</td><td>0.32</td><td>0.6129</td></tr>\n",
    "<tr><td>RL-Recall</td><td>0.4444</td><td>0.2962</td><td>0.3725</td></tr>\n",
    "<tr><td>RL-F1</td><td>0.3999</td><td>0.3076</td><td>0.4634</td></tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "<!-- Zweite Tabelle -->\n",
    "<div>\n",
    "<p style=\"text-align: center;\"><strong>Originale Modelle</strong></p>\n",
    "<table>\n",
    "<tr>\n",
    "<th>ROUGE-SCORE</th><th>mt5-small</th><th>t5-small</th><th>mbart-large-50</th>\n",
    "</tr>\n",
    "<tr><td>R1-Precision</td><td>0.35</td><td>0.56</td><td>0.4</td></tr>\n",
    "<tr><td>R1-Recall</td><td>0.2592</td><td>0.5185</td><td>0.7450</td></tr>\n",
    "<tr><td>R1-F1</td><td>0.2978</td><td>0.5384</td><td>0.5205</td></tr>\n",
    "<tr><td>R2-Precision</td><td>0.1578</td><td>0.375</td><td>0.2978</td></tr>\n",
    "<tr><td>R2-Recall</td><td>0.1153</td><td>0.3461</td><td>0.56</td></tr>\n",
    "<tr><td>R2-F1</td><td>0.1333</td><td>0.3599</td><td>0.3888</td></tr>\n",
    "<tr><td>RL-Precision</td><td>0.35</td><td>0.52</td><td>0.3789</td></tr>\n",
    "<tr><td>RL-Recall</td><td>0.2592</td><td>0.4814</td><td>0.7058</td></tr>\n",
    "<tr><td>RL-F1</td><td>0.2978</td><td>0.5</td><td>0.4931</td></tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204abbd7-2d34-4423-80a7-a23e8d3d93c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Vergleich mit anderen Modellen:**\n",
    "\n",
    "Originaltext: df_test_csv[\"speechContent\"][0]<br>\n",
    "\n",
    "**Was man hier sehen kann ist, dass besonders das optimierte mt5-small Modell um einiges besser ist als das originale mt5-small Modell.**\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "\n",
    "<!-- Erste Tabelle -->\n",
    "<div>\n",
    "<p style=\"text-align: center;\"><strong>Finetuned Modelle</strong></p>\n",
    "<table>\n",
    "<tr>\n",
    "<th>ROUGE-SCORE</th><th>mt5-small</th><th>t5-small</th><th>mbart-large-50</th>\n",
    "</tr>\n",
    "<tr><td>R1-Precision</td><td>0.6060</td><td>0.6774</td><td>0.6363</td></tr>\n",
    "<tr><td>R1-Recall</td><td>0.7407</td><td>0.7777</td><td>0.7777</td></tr>\n",
    "<tr><td>R1-F1</td><td>0.6666</td><td>0.7241</td><td>0.7</td></tr>\n",
    "<tr><td>R2-Precision</td><td>0.4375</td><td>0.4666</td><td>0.4687</td></tr>\n",
    "<tr><td>R2-Recall</td><td>0.5384</td><td>0.5384</td><td>0.5769</td></tr>\n",
    "<tr><td>R2-F1</td><td>0.4827</td><td>0.5</td><td>0.5172</td></tr>\n",
    "<tr><td>RL-Precision</td><td>0.5151</td><td>0.5806</td><td>0.5757</td></tr>\n",
    "<tr><td>RL-Recall</td><td>0.6296</td><td>0.6666</td><td>0.7037</td></tr>\n",
    "<tr><td>RL-F1</td><td>0.5666</td><td>0.6206</td><td>0.6333</td></tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "<!-- Zweite Tabelle -->\n",
    "<div>\n",
    "<p style=\"text-align: center;\"><strong>Originale Modelle</strong></p>\n",
    "<table>\n",
    "<tr>\n",
    "<th>ROUGE-SCORE</th><th>mt5-small</th><th>t5-small</th><th>mbart-large-50</th>\n",
    "</tr>\n",
    "<tr><td>R1-Precision</td><td>0.1</td><td>0.6176</td><td>0.4772</td></tr>\n",
    "<tr><td>R1-Recall</td><td>0.1111</td><td>0.7777</td><td>0.7777</td></tr>\n",
    "<tr><td>R1-F1</td><td>0.1052</td><td>0.6885</td><td>0.5915</td></tr>\n",
    "<tr><td>R2-Precision</td><td>0.0</td><td>0.4545</td><td>0.3488</td></tr>\n",
    "<tr><td>R2-Recall</td><td>0.0</td><td>0.5769</td><td>0.5769</td></tr>\n",
    "<tr><td>R2-F1</td><td>0.0</td><td>0.5084</td><td>0.4347</td></tr>\n",
    "<tr><td>RL-Precision</td><td>0.1</td><td>0.5294</td><td>0.4090</td></tr>\n",
    "<tr><td>RL-Recall</td><td>0.1111</td><td>0.6666</td><td>0.6666</td></tr>\n",
    "<tr><td>RL-F1</td><td>0.1052</td><td>0.5901</td><td>0.5070</td></tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b78d9-b1d5-4bde-8f56-9fd22768cdff",
   "metadata": {},
   "source": [
    "Aber nicht immer schneidet das finetuned Modell besser oder viel besser ab:\n",
    "\n",
    "**df_test_csv[\"speechContent\"][255]**\n",
    "\n",
    "Fine-tuned Modell ROUGE Scores:<br>\n",
    "ROUGE-1: Score(precision=0.3333333333333333, recall=0.4074074074074074, fmeasure=0.36666666666666664)<br>\n",
    "ROUGE-2: Score(precision=0.125, recall=0.15384615384615385, fmeasure=0.13793103448275862)<br>\n",
    "ROUGE-L: Score(precision=0.24242424242424243, recall=0.2962962962962963, fmeasure=0.26666666666666666)<br>\n",
    "\n",
    "Originales Modell ROUGE Scores:<br>\n",
    "ROUGE-1: Score(precision=0.43333333333333335, recall=0.48148148148148145, fmeasure=0.456140350877193)<br>\n",
    "ROUGE-2: Score(precision=0.13793103448275862, recall=0.15384615384615385, fmeasure=0.14545454545454548)<br>\n",
    "ROUGE-L: Score(precision=0.26666666666666666, recall=0.2962962962962963, fmeasure=0.28070175438596495)<br>\n",
    "\n",
    "Originaltext:<br>\n",
    "ich habe das erst vor wenigen tagen in einem umfangreichen artikel in der zeitung ‚Äûdie welt\" gemacht. wir machen das in vielen fachzeitschriften. je nach bedarf wird dies fortgesetzt.\n",
    "\n",
    "Referenzzusammenfassung:<br>\n",
    "Zusammenfassung: \"Vor kurzem habe ich einen ausf√ºhrlichen Artikel in der 'Welt' ver√∂ffentlicht. Wir publizieren regelm√§√üig in Fachzeitschriften und setzen dies je nach Bedarf fort.\"\n",
    "\n",
    "Originales Modell:<br>\n",
    "ich habe das erst vor wenigen tagen in einem umfangreichen artikel in der zeitung ‚Äûdie welt\" gemacht. je nach bedarf wird dies dies fortgesetzt. ich mache das in vielen fachzeitschriften.\n",
    "\n",
    "FN-Modell:<br>\n",
    "in der Zeitung ‚Äûdie welt‚Äú wird ein umfangreiches Artikel erstellt, das in vielen fachzeitschriften gedruckt wird. Die Fortsetzungen werden nach Bedarf gesetzt. Die Zeitschriften werden in der aktuellen Zeitschrift ‚Äûdie Welt‚Äú ver√∂ffentlicht.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b5aad-e762-478b-8d1f-eafe0e55ac1f",
   "metadata": {},
   "source": [
    "M√∂gliche Gr√ºnde daf√ºr k√∂nnen sein:\n",
    "- **Mangel an Daten**: Mit zu wenigen Trainingsdaten kann das Modell nicht ausreichend lernen und hat Schwierigkeiten bei der Generalisierung.\n",
    "- **Qualit√§t der Referenzzusammenfassungen**: Da die Referenzzusammenfassungen keine menschlichen Zusammenfassungen sind, sondern generierte, k√∂nnten sie weniger genau oder verst√§ndlich sein, was die Bewertung des Modells beeintr√§chtigen kann \n",
    "- **Overfitting**: Einige Testdaten √§hneln vielleicht den Trainingsdaten, und das Modell k√∂nnte sich zu stark an diese spezifischen Beispiele anpassen, wodurch die Generalisierungsf√§higkeit auf neue, unterschiedliche Daten eingeschr√§nkt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b7a9a-083a-458e-942e-c3f85fcc4c20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bemerkung\n",
    "Was uns aufgefallen ist, dass unser optimiertes (`t5-small`) Modell auch f√ºr einige Texte bessere Zusammenfassungen erstellt als das originale Modell, selbst wenn der Inhalt nicht viel oder garnichts mit dem trainierten Kontext der Datens√§tze zu tun hat.\n",
    "\n",
    "*Ein Beispiel verdeutlicht diesen Unterschied:*\n",
    "\n",
    "Originaltext:<br>\n",
    "Die Text-Extraction (englisch text extraction auch englisch keyphrase extraction) bzw. Textextrahierung ist eine Methode zur automatischen Zusammenfassung eines Textes mit Hilfe computerlinguistischer Techniken. Dabei werden Teile eines Textes ‚Äì zum Beispiel S√§tze oder ganze Abschnitte ‚Äì mittels statistischer und/oder heuristischer Methoden bez√ºglich ihrer Wichtigkeit oder Relevanz bewertet. Diese scores of importance dienen als Grundlage f√ºr die Entscheidung, welche Teile (\"keyphrases\") extrahiert und zu einem k√ºrzeren Text zusammengestellt werden, der dann einen √úberblick √ºber die Inhalte des Originaltextes bietet und in der Regel als extract oder abstract bezeichnet wird.\n",
    "\n",
    "Nach Karen Sp√§rck Jones (1999) haben die mit dieser Methode produzierten Zusammenfassungen den Nachteil, dass sie zumeist wenig koh√§rent und somit nur schlecht lesbar und unter Umst√§nden sogar unverst√§ndlich sind. Andererseits ist diese Methode und ihre Varianten vermutlich einfacher in automatischen Systemen zu modellieren. Beispiele daf√ºr sind die Systeme von Hans Peter Luhn (1959) (Extraktionsalgorithmus nach Luhn) und Edmundson (1969) und die Ans√§tze von Rath et al. (1961) und Brandow et al. (1995).\n",
    "\n",
    "Zusammenfassung des ungetunten Modells (T5-Small, pretrained):<br>\n",
    "‚ÄûDabei werden Teile eines Textes mittels statistischer und/oder heuristischer Techniken bewertet. Diese Scores of Importance dienen als Grundlage f√ºr die Entscheidung, welche Teile (Keyphrases) extrahiert und zu einem k√ºrzeren Text zusammengestellt werden, der dann einen √úberblick √ºber die Inhalte des Originaltextes bietet und in der Regel als Extract oder Abstract bezeichnet wird.‚Äú\n",
    "\n",
    "Zusammenfassung des getunten Modells:<br>\n",
    "‚ÄûText-Extraction (englisch text extraction) bzw. Textextrahierung ist eine Methode zur automatischen Zusammenfassung von Texten mit Hilfe computerlinguistischer Techniken. Teile einer Texte werden mittels statistischer und/oder heuristischer Methoden bewertet, die einen √úberblick √ºber die Inhalte des Originaltextes bieten und in der Regel als Extract oder Abstract bezeichnet werden. Nach Karen Sp√§rck Jones (1999) hat die Methode erstellt Zusammenfassungen, die wenig koh√§rent sind und die Ans√§tze von Rath et al.‚Äú\n",
    "\n",
    "Der Unterschied ist deutlich:<br> Das vortrainierte Modell produziert eine eher generische und redundante Zusammenfassung, w√§hrend das getunte Modell die Begriffe pr√§ziser einordnet, den Kontext besser versteht und koh√§rentere S√§tze formuliert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a070a-bbf5-4521-99b5-8322fba685e7",
   "metadata": {},
   "source": [
    "## Vergleich: extr. und abstr. Summarizer\n",
    "\n",
    "F√ºr die extraktive Textzusammenfassung setzen wir unter anderem die Python-Bibliothek Sumy ein. Die St√§rke von Sumy liegt in seiner einfachen Implementierung und der M√∂glichkeit, unterschiedliche Methoden f√ºr extraktive Zusammenfassungen zu testen. Dies macht es zu einem idealen Werkzeug f√ºr unsere Analyse von Redebeitr√§gen aus dem Open-Discourse-Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "id": "56ccc459-7676-4d8c-9689-9f145ae0482f",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-13T13:34:28.422978Z",
     "start_time": "2025-02-13T13:34:28.419467Z"
    }
   },
   "source": "text = speech_contents[609]",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "5d5c7b0a-0bc3-4d05-87d0-55656c816ae9",
   "metadata": {},
   "source": [
    "Als N√§chstes laden wir die `punkt`-Ressource von **NLTK**, die f√ºr die Tokenisierung von S√§tzen und W√∂rtern ben√∂tigt wird.  \n",
    "Diese erlaubt es uns, Texte in einzelne W√∂rter oder S√§tze zu zerlegen."
   ]
  },
  {
   "cell_type": "code",
   "id": "98b05523-66a6-4828-a4b1-aa1decea343a",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-13T13:38:54.280910Z",
     "start_time": "2025-02-13T13:38:53.233007Z"
    }
   },
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dolov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dolov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "4eeb44f3-60c8-40cf-9461-8431bed9e2f9",
   "metadata": {},
   "source": [
    "Als N√§chstes verwenden wir **Sumy**, um eine extraktive Zusammenfassung mit dem **LexRank-Algorithmus** zu erstellen.  \n",
    "LexRank ist ein graphbasiertes Verfahren zur Bestimmung der wichtigsten S√§tze in einem Text."
   ]
  },
  {
   "cell_type": "code",
   "id": "aa7da8de-58cb-407c-af2d-5936e6a97a2d",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-13T13:33:47.917893Z",
     "start_time": "2025-02-13T13:33:22.298277Z"
    }
   },
   "source": [
    "!pip install sumy"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting breadability>=0.1.20 (from sumy)\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from sumy) (2.32.3)\n",
      "Collecting pycountry>=18.2.23 (from sumy)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: nltk>=3.0.2 in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from sumy) (3.9.1)\n",
      "Collecting chardet (from breadability>=0.1.20->sumy)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting lxml>=2.0 (from breadability>=0.1.20->sumy)\n",
      "  Downloading lxml-5.3.1-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: click in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from requests>=2.7.0->sumy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from requests>=2.7.0->sumy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from requests>=2.7.0->sumy) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dolov\\onedrive\\desktop\\phyton_udemy\\abschnitt6funktionen\\.venv\\lib\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n",
      "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 2.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.8/6.3 MB 2.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.3/6.3 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.1/6.3 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.1/6.3 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 3.9/6.3 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading lxml-5.3.1-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.4/3.8 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Building wheels for collected packages: breadability, docopt\n",
      "  Building wheel for breadability (pyproject.toml): started\n",
      "  Building wheel for breadability (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21801 sha256=1c31efa02a9cff387f8835d0e2e8bc2d64adcf66f7a9dd8c43029a8ac0df4dd3\n",
      "  Stored in directory: c:\\users\\dolov\\appdata\\local\\pip\\cache\\wheels\\32\\99\\64\\59305409cacd03aa03e7bddf31a9db34b1fa7033bd41972662\n",
      "  Building wheel for docopt (pyproject.toml): started\n",
      "  Building wheel for docopt (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13822 sha256=6358a4363e46ef7884f7a07852455f079360746daed3874b33e978be0e64e81b\n",
      "  Stored in directory: c:\\users\\dolov\\appdata\\local\\pip\\cache\\wheels\\1a\\bf\\a1\\4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "Successfully built breadability docopt\n",
      "Installing collected packages: docopt, pycountry, lxml, chardet, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 chardet-5.2.0 docopt-0.6.2 lxml-5.3.1 pycountry-24.6.1 sumy-0.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "47e5ea0f-33fb-41dd-bf13-b49d31847b17",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-13T13:38:58.650650Z",
     "start_time": "2025-02-13T13:38:58.527896Z"
    }
   },
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "\n",
    "# Parser und Tokenizer\n",
    "parser = PlaintextParser.from_string(text, Tokenizer(\"german\"))\n",
    "summarizer = LexRankSummarizer()\n",
    "\n",
    "# Zusammenfassung erstellen (z. B. 3 S√§tze)\n",
    "summary = summarizer(parser.document, 3)\n",
    "\n",
    "extractive_sum=\"\"\n",
    "for sentence in summary:\n",
    "    extractive_sum += str(sentence)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "b761f09d-fb55-4db5-889a-9ae15202805a",
   "metadata": {},
   "source": [
    "F√ºr die abstraktive Textzusammenfassung nutzen wir das vortrainierte Modell t5-small. Die St√§rke von T5 liegt in seiner F√§higkeit, den Inhalt eines Textes neu zu formulieren und pr√§gnant wiederzugeben, anstatt lediglich wichtige S√§tze auszuw√§hlen. Dies macht es zu einem leistungsf√§higen Werkzeug f√ºr unsere Analyse von Redebeitr√§gen aus dem Open-Discourse-Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "id": "ae4a66fe-a06c-4541-a931-96b6b11c3f81",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-13T13:39:15.547918Z",
     "start_time": "2025-02-13T13:39:04.185394Z"
    }
   },
   "source": [
    "from transformers import pipeline, T5Tokenizer\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"models_summarizer/t5_finetuned_abgabe/summarizer_model\", tokenizer=\"models_summarizer/t5_finetuned_abgabe/summarizer_model\")\n",
    "\n",
    "abstractive_sum = summarizer(text, max_length=250, min_length=50, do_sample=False)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (920 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "59e355da-230c-4f3e-8d7d-331557ddb6e1",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-13T13:39:15.557076Z",
     "start_time": "2025-02-13T13:39:15.552985Z"
    }
   },
   "source": [
    "print(\"Original text: \", text)\n",
    "print(\"Extraktive Zusammenfassung: \", extractive_sum)\n",
    "print()\n",
    "print(\"Abstraktive Zusammenfassung: \", abstractive_sum)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  Nein; ich, sagte: bei der bekannten Loyalit√§t rechnete ich damit, da√ü Sie das nicht t√§ten, Herr Pr√§sident.\n",
      "Aber nun zu dem Thema selbst. Ich m√∂chte darauf aufmerksam machen, da√ü nach meiner Meinung das Berlin-Thema, das wir hier nun verschiedentlich er√∂rtert haben, bisher etwas zu kasuistisch angefa√üt worden ist. Man hat heute vom Herrn Finanzminister eine Zusammenfassung verschiedener Hilfsma√ünahmen geh√∂rt, allerdings nicht so ersch√∂pfend - oder sollte es mir entgangen sein? -, da√ü man von allem, auch zum Beispiel von der Portoabgabe ein zahlenm√§√üiges Ergebnis h√§tte h√∂ren k√∂nnen. Das ist aber nur die eine Seite. Das betrifft n√§mlich die Ma√ünahmen, die zum Schutz und zur Unterst√ºtzung der k√§mpfenden Berliner Bev√∂lkerung beschlossen worden sind. Es bed√ºrfte einmal einer systematischen Zusammenstellung, damit wir √ºbersehen, was bisher insgesamt, wie auch was eus den einzelnen Quellen nach Berlin geflossen ist.\n",
      "Wichtiger scheint mir die andere Seite zu sein. Wir leiden -- so scheint es mir -- ein wenig daran, da√ü wir die ganze Anlegenheit Berlin am Symptom kurieren wollen, ohne da√ü ein einheitlicher, gro√üz√ºgiger Plan vorliegt, nach welchem man vorgehen kann und in Zukunft auch vorgehen will. Man mu√ü eimal die Notwendigkeiten von Berlin und ebenso demgegen√ºber die eigenen Bed√ºrfnisse feststellen und dann abw√§gen; denn schlie√ülich gibt es nicht nur in Berlin Arbeitslose, sondern leider Gottes ist die Arbeitslosigkeit eine Angelegenheit, die auch im Westen bei den bekannterma√üen 1,8 Millionen direkt Betroffenen die schwersten Sorgen hervorruft. Bekanntlich gibt es nicht blo√ü in Berlin, sondern auch im Westen Ausgebombte. Dazu haben wir noch Fl√ºchtlinge und au√üerdem die, die allt√§glich √ºber die Zonengrenze von der russisch besetzten Zone her durchsickern. Es mu√ü doch einmal ein Gesamtbild gegeben und ein Plan aufgestellt werden, um bei/ des miteinander abzustimmen.\n",
      "Auch f√ºr die Reorganisation der Berliner Wirtschaft und f√ºr die Abstimmung ihrer Leistungen mit den Bed√ºrfnissen des Westens mu√ü ein Plan gemacht werden. Wenn wir nur Geld dahin leiten, besteht die Gefahr, da√ü dieses dort durchaus fehl investiert wird, so da√ü es weder der Berliner noch unserer Wirtschaft nutzt, sondern da√ü wom√∂glich hinterher die beiden Wirtschaften, die aufeinander angewiesen sind und sich gegenseitig unterst√ºtzen sollen, noch gegeneinanderarbeiten. Unsere Fraktion vermi√üt einen solchen grundlegenden und weitsichtigen Plan, der es uns erm√∂glicht, festzustellen, ob und in welcher Weise das dahin geschickte Geld auch produktiv wirkt und den gr√∂√üten Nutzeffekt f√ºr beide Teile, f√ºr den gebenden und den nehmenden Teil, gew√§hrleistet.\n",
      "Nebenbei bemerkt w√ºrde ein solcher Plan sowohl √ºber die Ausgaben wie √ºber die Einnahmen geeignet sein, die Augen der Welt einmal darauf zu lenken, da√ü die Unterst√ºtzung von Berlin nicht allein Sache Westdeutschlands sein darf. In erster Linie ist es nat√ºrlich unsere Angelegenheit, da die Berliner Br√ºder unseres Volkes sind, die wegen ihrer und unserer Freiheit leiden. Zum andern ist es aber nicht allein unsere Schuld, da√ü diese Situation besteht. An den Reibungen unter\n",
      "\n",
      "({0})\n",
      "den Alliierten sind die Deutschen nicht schuld. Deswegen m√ºssen wir durch die Darlegung der Vern√ºnftigkeit unserer Ma√ünahmen und durch den Hinweis darauf, da√ü wir bis an das √Ñu√üerste unserer Leistungsf√§higkeit gegangen sind und uns hinsichtlich unserer Leistungen auch nicht selber √ºberziehen d√ºrfen, die Welt f√ºr den Gedanken erw√§rmen, da√ü die Unterst√ºtzung von Berlin eine Angelegenheit der Welt√∂ffentlichkeit und nicht nur eine deutsche Angelegenheit ist.\n",
      "({1})\n",
      "\n",
      "Extraktive Zusammenfassung:  Das ist aber nur die eine Seite.Wichtiger scheint mir die andere Seite zu sein.Deswegen m√ºssen wir durch die Darlegung der Vern√ºnftigkeit unserer Ma√ünahmen und durch den Hinweis darauf, da√ü wir bis an das √Ñu√üerste unserer Leistungsf√§higkeit gegangen sind und uns hinsichtlich unserer Leistungen auch nicht selber √ºberziehen d√ºrfen, die Welt f√ºr den Gedanken erw√§rmen, da√ü die Unterst√ºtzung von Berlin eine Angelegenheit der Welt√∂ffentlichkeit und nicht nur eine deutsche Angelegenheit ist.\n",
      "\n",
      "Abstraktive Zusammenfassung:  [{'summary_text': 'In seiner Rede betont der Redner die Notwendigkeit einer einheitlichen, gro√üz√ºgigen Plan zur Unterst√ºtzung der k√§mpfenden Berliner Bev√∂lkerung. Er hebt hervor, dass die Arbeitslosigkeit eine Angelegenheit ist, die im Westen bei 1,8 Millionen direkt Betroffenen hervorruft. Zudem kritisiert er die Bedeutung eines umfassenden Planes f√ºr die Reorganisation der Berliner Wirtschaft und die Abstimmung ihrer Leistungen mit den Bed√ºrfnissen des Westens. Der Redner betont, dass ein solcher Plan nicht alleine Verantwortung f√ºr die Unterst√ºtzung von Berlin tragen muss, um die u√üerungen zu korrigieren und fordert eine einheitliche Zusammenarbeit zwischen Deutschland und Deutschland. Er warnt vor der Gefahr, dass es nicht nur in Deutschland angesiedelt ist, sondern auch in der Lage ist, sich gegenseitig zu verhalten, und betont die Bedeutung der Unterst√ºtzung der Deutschen, die die Fl√ºchtlinge und die Auswirkungen auf die Bev√∂lkerung hat, insbesondere in der Reibungen, er fordert ein umfassendes Plan, der die Unterst√ºtzung der Welt f√ºr die Zukunft zu gew√§hrleisten.'}]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "a6d25c4d-cd21-4225-b019-9a32ac420e74",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "\n",
    "Zu Beginn des Projekts hatten wir gro√üe Schwierigkeiten und mussten viel recherchieren, um die notwendigen Kenntnisse zu erhalten. Eine der gr√∂√üten Herausforderungen war die Datenbeschaffung, da wir keinen Datensatz finden konnten, der eine Referenzzusammenfassung enthielt. Dadurch mussten wir uns eigene Daten zusammenstellen, was den gesamten Prozess deutlich komplizierter machte. Zudem bemerkten wir, dass die Menge der verf√ºgbaren Daten nicht ausreichte, um die Modelle effektiv nach unseren Vorstellungen zu trainieren. Dies wirkte sich etwas auf die Qualit√§t der Ergebnisse aus, insbesondere da die Qualit√§t der generierten Referenzzusammenfassungen den Erfolg des Fine-Tunings beeinflusste.\n",
    "\n",
    "Trotz dieser Probleme haben wir im Verlauf des Projekts eine Menge gelernt und wichtige Erfahrungen gesammelt. Besonders im Bereich der Textzusammenfassung, beim Fine-Tuning von Modellen und bei der Aufbereitung von Daten konnten wir unser Wissen erweitern. Obwohl es nicht ganz unseren urspr√ºnglichen Vorstellungen eines deutlich genaueren und pr√§ziseren Summarizers entsprach, haben uns die Herausforderungen geholfen, besser zu verstehen wie wichtig gute Datens√§tze sind, um wirklich gute Ergebnisse zu erzielen."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Quellen\n",
    "\n",
    "- [Two minutes NLP ‚Äî Learn the ROUGE metric by examples](https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499)\n",
    "- [Understanding BLEU and ROUGE score for NLP evaluation](https://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb)\n",
    "- [Extractive vs. Abstractive Summarization](https://www.prodigaltech.com/blog/extractive-vs-abstractive-summarization-how-does-it-work#foldMain)\n",
    "- [Fine-tune a pretrained model](https://huggingface.co/docs/transformers/en/training)\n",
    "- [How to Calculate ROUGE Score in Python](https://thepythoncode.com/article/calculate-rouge-score-in-python#setting-up-the-environment-for-python-implementation)"
   ],
   "id": "6a7c791106f112a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55772e6-278b-431f-b0a3-9e9e317aa50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
